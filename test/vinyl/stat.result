test_run = require('test_run').new()
---
...
-- Since we store LSNs in data files, the data size may differ
-- from run to run. Deploy a new server to make sure it will be
-- the same so that we can check it.
test_run:cmd('create server test with script = "vinyl/stat.lua"')
---
- true
...
test_run:cmd('start server test')
---
- true
...
test_run:cmd('switch test')
---
- true
...
fiber = require('fiber')
---
...
s = box.schema.space.create('test', {engine = 'vinyl'})
---
...
_ = s:create_index('pk', {page_size = 4096, range_size = 16384, run_count_per_level = 1, run_size_ratio = 1000})
---
...
--
-- Helper functions.
--
test_run:cmd("setopt delimiter ';'")
---
- true
...
-- Generate random 1K padding.
function pad()
    local t = {}
    for i = 1, 1024 do
        t[i] = string.char(math.random(65, 90))
    end
    return table.concat(t)
end;
---
...
-- Insert a tuple into the test space.
function put(val)
    box.space.test:replace{val, pad()}
end;
---
...
-- Compute the difference between two tables containing stats.
-- If a field value is the same, it will be set to nil in the
-- resulting table. If 'path' is not 'nil', compare statistics
-- starting from 'path'.
function stat_diff(stat1, stat2, path)
    while path ~= nil and path ~= '' do
        local i = path:find('%.') or path:len() + 1
        local node = path:sub(1, i - 1)
        path = path:sub(i + 1, path:len())
        stat1 = stat1[node]
        stat2 = stat2[node]
    end
    if type(stat1) == 'string' then
        return nil
    end
    if type(stat1) == 'number' then
        return stat1 ~= stat2 and stat1 - stat2 or nil
    end
    assert(type(stat1) == 'table')
    local diff
    for k, v1 in pairs(stat1) do
        local v2 = stat2[k]
        local d = stat_diff(v1, v2)
        if d ~= nil then
            if diff == nil then
                diff = {}
            end
            diff[k] = d
        end
    end
    return diff
end;
---
...
-- Return index statistics.
--
-- Note, latency measurement is beyond the scope of this test
-- so we just filter it out.
--
-- Filter dump/compaction time as we need error injection to
-- test them properly.
function istat()
    local st = box.space.test.index.pk:stat()
    st.latency = nil
    st.disk.dump.time = nil
    st.disk.compaction.time = nil
    return st
end;
---
...
-- Return global statistics.
--
-- Note, checking correctness of the load regulator logic is beyond
-- the scope of this test so we just filter out related statistics.
--
-- Filter dump/compaction time as we need error injection to
-- test them properly.
function gstat()
    local st = box.stat.vinyl()
    st.regulator = nil
    st.scheduler.dump_time = nil
    st.scheduler.compaction_time = nil
    st.memory.level0 = nil
    return st
end;
---
...
-- Wait until a stat counter changes.
function wait(stat_func, stat, path, diff)
    while (stat_diff(stat_func(), stat, path) or 0) < diff do
        fiber.sleep(0.01)
    end
end;
---
...
test_run:cmd("setopt delimiter ''");
---
- true
...
-- initially stats are empty
istat()
---
- range_count: 1
  lookup: 0
  cache:
    invalidate:
      rows: 0
      bytes: 0
    index_size: 0
    rows: 0
    evict:
      rows: 0
      bytes: 0
    put:
      rows: 0
      bytes: 0
    lookup: 0
    bytes: 0
    get:
      rows: 0
      bytes: 0
  run_histogram: '[0]:1'
  disk:
    last_level:
      bytes_compressed: 0
      pages: 0
      rows: 0
      bytes: 0
    rows: 0
    statement:
      inserts: 0
      replaces: 0
      upserts: 0
      deletes: 0
    dump:
      input:
        rows: 0
        bytes: 0
      output:
        bytes_compressed: 0
        pages: 0
        rows: 0
        bytes: 0
      count: 0
    bloom_size: 0
    index_size: 0
    iterator:
      read:
        bytes_compressed: 0
        pages: 0
        rows: 0
        bytes: 0
      bloom:
        hit: 0
        miss: 0
      lookup: 0
      get:
        rows: 0
        bytes: 0
    compaction:
      input:
        bytes_compressed: 0
        pages: 0
        rows: 0
        bytes: 0
      queue:
        bytes_compressed: 0
        pages: 0
        rows: 0
        bytes: 0
      output:
        bytes_compressed: 0
        pages: 0
        rows: 0
        bytes: 0
      count: 0
    pages: 0
    bytes_compressed: 0
    bytes: 0
  range_size: 16384
  rows: 0
  run_avg: 0
  dumps_per_compaction: 0
  upsert:
    squashed: 0
    applied: 0
  bytes: 0
  put:
    rows: 0
    bytes: 0
  skip:
    rows: 0
    bytes: 0
  run_count: 0
  txw:
    bytes: 0
    rows: 0
    iterator:
      lookup: 0
      get:
        rows: 0
        bytes: 0
  memory:
    bytes: 0
    index_size: 0
    rows: 0
    iterator:
      lookup: 0
      get:
        rows: 0
        bytes: 0
  get:
    rows: 0
    bytes: 0
...
gstat()
---
- tx:
    conflict: 0
    commit: 0
    rollback: 0
    statements: 0
    transactions: 0
    gap_locks: 0
    read_views: 0
  memory:
    tuple_cache: 0
    tx: 0
    bloom_filter: 0
    page_index: 0
    tuple: 0
  disk:
    data_compacted: 0
    data: 0
    index: 0
  scheduler:
    tasks_inprogress: 0
    dump_output: 0
    compaction_queue: 0
    compaction_output: 0
    dump_count: 0
    tasks_failed: 0
    tasks_completed: 0
    dump_input: 0
    compaction_input: 0
...
box.stat.vinyl().memory.level0 == 0
---
- true
...
--
-- Index statistics.
--
-- Compressed data size may differ as padding is random.
-- Besides, it may depend on the zstd version so let's
-- filter it out.
test_run:cmd("push filter 'bytes_compressed: .*' to 'bytes_compressed: <bytes_compressed>'")
---
- true
...
-- put + dump
st = istat()
---
...
for i = 1, 100, 4 do put(i) end
---
...
box.snapshot()
---
- ok
...
wait(istat, st, 'disk.dump.count', 1)
---
...
stat_diff(istat(), st)
---
- put:
    rows: 25
    bytes: 26325
  rows: 25
  run_avg: 1
  run_count: 1
  dumps_per_compaction: 1
  disk:
    last_level:
      bytes: 26049
      pages: 7
      bytes_compressed: <bytes_compressed>
      rows: 25
    rows: 25
    statement:
      replaces: 25
    dump:
      input:
        rows: 25
        bytes: 26325
      count: 1
      output:
        bytes: 26049
        pages: 7
        bytes_compressed: <bytes_compressed>
        rows: 25
    bytes: 26049
    index_size: 350
    pages: 7
    bytes_compressed: <bytes_compressed>
    bloom_size: 70
  bytes: 26049
...
-- put + dump + compaction
st = istat()
---
...
for i = 1, 100, 2 do put(i) end
---
...
box.snapshot()
---
- ok
...
wait(istat, st, 'disk.compaction.count', 1)
---
...
stat_diff(istat(), st)
---
- put:
    rows: 50
    bytes: 52650
  bytes: 26042
  disk:
    last_level:
      bytes: 26042
      pages: 6
      bytes_compressed: <bytes_compressed>
      rows: 25
    rows: 25
    statement:
      replaces: 25
    dump:
      input:
        rows: 50
        bytes: 52650
      count: 1
      output:
        bytes: 52091
        pages: 13
        bytes_compressed: <bytes_compressed>
        rows: 50
    bytes: 26042
    index_size: 300
    pages: 6
    bytes_compressed: <bytes_compressed>
    compaction:
      input:
        bytes: 78140
        pages: 20
        bytes_compressed: <bytes_compressed>
        rows: 75
      count: 1
      output:
        bytes: 52091
        pages: 13
        bytes_compressed: <bytes_compressed>
        rows: 50
  rows: 25
...
-- point lookup from disk + cache put
st = istat()
---
...
s:get(1) ~= nil
---
- true
...
stat_diff(istat(), st)
---
- cache:
    index_size: 49152
    rows: 1
    bytes: 1053
    lookup: 1
    put:
      rows: 1
      bytes: 1053
  disk:
    iterator:
      read:
        bytes: 4167
        pages: 1
        bytes_compressed: <bytes_compressed>
        rows: 4
      lookup: 1
      get:
        rows: 1
        bytes: 1053
  txw:
    iterator:
      lookup: 1
  memory:
    iterator:
      lookup: 1
  lookup: 1
  get:
    rows: 1
    bytes: 1053
...
-- point lookup from cache
st = istat()
---
...
s:get(1) ~= nil
---
- true
...
stat_diff(istat(), st)
---
- cache:
    lookup: 1
    put:
      rows: 1
      bytes: 1053
    get:
      rows: 1
      bytes: 1053
  txw:
    iterator:
      lookup: 1
  lookup: 1
  get:
    rows: 1
    bytes: 1053
...
-- put in memory + cache invalidate
st = istat()
---
...
put(1)
---
...
stat_diff(istat(), st)
---
- cache:
    invalidate:
      rows: 1
      bytes: 1053
    rows: -1
    bytes: -1053
  rows: 1
  memory:
    index_size: 49152
    bytes: 1053
    rows: 1
  put:
    rows: 1
    bytes: 1053
  bytes: 1053
...
-- point lookup from memory
st = istat()
---
...
s:get(1) ~= nil
---
- true
...
stat_diff(istat(), st)
---
- cache:
    bytes: 1053
    lookup: 1
    rows: 1
    put:
      rows: 1
      bytes: 1053
  txw:
    iterator:
      lookup: 1
  lookup: 1
  memory:
    iterator:
      lookup: 1
      get:
        rows: 1
        bytes: 1053
  get:
    rows: 1
    bytes: 1053
...
-- put in txw + point lookup from txw
st = istat()
---
...
box.begin()
---
...
put(1)
---
...
s:get(1) ~= nil
---
- true
...
stat_diff(istat(), st)
---
- txw:
    rows: 1
    bytes: 1053
    iterator:
      lookup: 1
      get:
        rows: 1
        bytes: 1053
  lookup: 1
  get:
    rows: 1
    bytes: 1053
...
box.rollback()
---
...
-- apply upsert in txw
st = istat()
---
...
box.begin()
---
...
_ = s:replace{1}
---
...
_ = s:upsert({1}, {{'=', 2, pad()}})
---
...
stat_diff(istat(), st, 'upsert')
---
- squashed: 1
  applied: 1
...
box.rollback()
---
...
-- apply upsert on get
st = istat()
---
...
_ = s:upsert({5}, {{'=', 2, pad()}})
---
...
s:get(5) ~= nil
---
- true
...
stat_diff(istat(), st, 'upsert')
---
- applied: 1
...
-- cache eviction
assert(box.cfg.vinyl_cache < 100 * 1024)
---
- true
...
for i = 1, 100 do put(i) end
---
...
st = istat()
---
...
for i = 1, 100 do s:get(i) end
---
...
stat_diff(istat(), st, 'cache')
---
- rows: 14
  bytes: 14742
  evict:
    rows: 86
    bytes: 90558
  lookup: 100
  put:
    rows: 100
    bytes: 105300
...
-- range split
for i = 1, 100 do put(i) end
---
...
st = istat()
---
...
box.snapshot()
---
- ok
...
wait(istat, st, 'disk.compaction.count', 2)
---
...
st = istat()
---
...
st.range_count -- 2
---
- 2
...
st.run_count -- 2
---
- 2
...
st.run_avg -- 1
---
- 1
...
st.run_histogram -- [1]:2
---
- '[1]:2'
...
-- range lookup
for i = 1, 100 do put(i) end
---
...
box.begin()
---
...
for i = 1, 100, 2 do put(i) end
---
...
st = istat()
---
...
#s:select()
---
- 100
...
stat_diff(istat(), st)
---
- cache:
    rows: 13
    bytes: 13689
    evict:
      rows: 37
      bytes: 38961
    lookup: 1
    put:
      rows: 51
      bytes: 53703
  disk:
    iterator:
      read:
        bytes: 104299
        pages: 25
        bytes_compressed: <bytes_compressed>
        rows: 100
      lookup: 2
      get:
        rows: 100
        bytes: 105300
  txw:
    iterator:
      lookup: 1
      get:
        rows: 50
        bytes: 52650
  memory:
    iterator:
      lookup: 1
      get:
        rows: 100
        bytes: 105300
  lookup: 1
  get:
    rows: 100
    bytes: 105300
...
box.rollback()
---
...
-- range lookup from cache
assert(box.cfg.vinyl_cache > 10 * 1024)
---
- true
...
for i = 1, 100 do put(i) end
---
...
box.begin()
---
...
#s:select({}, {limit = 5})
---
- 5
...
st = istat()
---
...
#s:select({}, {limit = 5})
---
- 5
...
stat_diff(istat(), st)
---
- cache:
    lookup: 1
    put:
      rows: 5
      bytes: 5265
    get:
      rows: 5
      bytes: 5265
  txw:
    iterator:
      lookup: 1
  lookup: 1
  get:
    rows: 5
    bytes: 5265
...
box.rollback()
---
...
--
-- Global statistics.
--
-- dump and compaction totals
gstat().scheduler.dump_input == istat().disk.dump.input.bytes
---
- true
...
gstat().scheduler.dump_output == istat().disk.dump.output.bytes
---
- true
...
gstat().scheduler.compaction_input == istat().disk.compaction.input.bytes
---
- true
...
gstat().scheduler.compaction_output == istat().disk.compaction.output.bytes
---
- true
...
-- use memory
st = box.stat.vinyl().memory.level0
---
...
put(1)
---
...
diff = box.stat.vinyl().memory.level0 - st
---
...
diff > 1000 and diff < 1100
---
- true
...
-- use cache
st = gstat()
---
...
_ = s:get(1)
---
...
stat_diff(gstat(), st, 'memory.tuple_cache')
---
- 1101
...
s:delete(1)
---
...
-- rollback
st = gstat()
---
...
box.begin()
---
...
_ = s:insert{1}
---
...
box.rollback()
---
...
stat_diff(gstat(), st, 'tx')
---
- rollback: 1
...
-- conflict
st = gstat()
---
...
ch1 = fiber.channel(1)
---
...
ch2 = fiber.channel(1)
---
...
test_run:cmd("setopt delimiter ';'")
---
- true
...
_ = fiber.create(function()
    box.begin()
    s:insert{1}
    ch1:put(true)
    ch2:get()
    pcall(box.commit)
    ch1:put(true)
end);
---
...
test_run:cmd("setopt delimiter ''");
---
- true
...
ch1:get()
---
- true
...
_ = s:insert{1}
---
...
ch2:put(true)
---
- true
...
ch1:get()
---
- true
...
stat_diff(gstat(), st, 'tx')
---
- conflict: 1
  commit: 1
  rollback: 1
...
s:delete(1)
---
...
-- tx statements
st = gstat()
---
...
box.begin()
---
...
for i = 1, 10 do s:replace{i} end
---
...
stat_diff(gstat(), st, 'tx')
---
- statements: 10
  transactions: 1
...
box.rollback()
---
...
stat_diff(gstat(), st, 'tx')
---
- rollback: 1
...
-- transactions
st = gstat()
---
...
ch1 = fiber.channel(5)
---
...
ch2 = fiber.channel(5)
---
...
test_run:cmd("setopt delimiter ';'")
---
- true
...
for i = 1, 5 do
    fiber.create(function()
        box.begin()
        s:replace{i}
        ch1:put(true)
        ch2:get()
        box.rollback()
        ch1:put(true)
    end)
end;
---
...
test_run:cmd("setopt delimiter ''");
---
- true
...
for i = 1, 5 do ch1:get() end
---
...
stat_diff(gstat(), st, 'tx')
---
- statements: 5
  transactions: 5
...
for i = 1, 5 do ch2:put(true) end
---
...
for i = 1, 5 do ch1:get() end
---
...
stat_diff(gstat(), st, 'tx')
---
- rollback: 5
...
-- read view
st = gstat()
---
...
ch1 = fiber.channel(1)
---
...
ch2 = fiber.channel(1)
---
...
test_run:cmd("setopt delimiter ';'")
---
- true
...
_ = fiber.create(function()
    box.begin()
    s:select()
    ch1:put(true)
    ch2:get()
    pcall(box.commit)
    ch1:put(true)
end);
---
...
test_run:cmd("setopt delimiter ''");
---
- true
...
ch1:get()
---
- true
...
_ = s:insert{1}
---
...
stat_diff(gstat(), st, 'tx')
---
- transactions: 1
  gap_locks: 1
  commit: 1
  read_views: 1
...
ch2:put(true)
---
- true
...
ch1:get()
---
- true
...
stat_diff(gstat(), st, 'tx')
---
- commit: 2
...
s:delete(1)
---
...
-- gap locks
st = gstat()
---
...
box.begin()
---
...
_ = s:select({10}, {iterator = 'LT'})
---
...
_ = s:select({20}, {iterator = 'GT'})
---
...
stat_diff(gstat(), st, 'tx')
---
- transactions: 1
  gap_locks: 2
...
box.commit()
---
...
stat_diff(gstat(), st, 'tx')
---
- commit: 1
...
-- free tuples pinned by Lua
_ = nil
---
...
_ = collectgarbage()
---
...
-- box.stat.reset
box.stat.reset()
---
...
istat()
---
- range_count: 2
  lookup: 0
  cache:
    invalidate:
      rows: 0
      bytes: 0
    index_size: 49152
    rows: 13
    evict:
      rows: 0
      bytes: 0
    put:
      rows: 0
      bytes: 0
    lookup: 0
    bytes: 13689
    get:
      rows: 0
      bytes: 0
  run_histogram: '[1]:2'
  disk:
    last_level:
      bytes_compressed: <bytes_compressed>
      pages: 25
      rows: 100
      bytes: 104299
    rows: 100
    statement:
      inserts: 0
      replaces: 100
      upserts: 0
      deletes: 0
    dump:
      input:
        rows: 0
        bytes: 0
      output:
        bytes_compressed: <bytes_compressed>
        pages: 0
        rows: 0
        bytes: 0
      count: 0
    bloom_size: 140
    index_size: 1250
    iterator:
      read:
        bytes_compressed: <bytes_compressed>
        pages: 0
        rows: 0
        bytes: 0
      bloom:
        hit: 0
        miss: 0
      lookup: 0
      get:
        rows: 0
        bytes: 0
    compaction:
      input:
        bytes_compressed: <bytes_compressed>
        pages: 0
        rows: 0
        bytes: 0
      queue:
        bytes_compressed: <bytes_compressed>
        pages: 0
        rows: 0
        bytes: 0
      output:
        bytes_compressed: <bytes_compressed>
        pages: 0
        rows: 0
        bytes: 0
      count: 0
    pages: 25
    bytes_compressed: <bytes_compressed>
    bytes: 104299
  range_size: 16384
  rows: 306
  run_avg: 1
  dumps_per_compaction: 1
  upsert:
    squashed: 0
    applied: 0
  bytes: 316082
  put:
    rows: 0
    bytes: 0
  skip:
    rows: 0
    bytes: 0
  run_count: 2
  txw:
    bytes: 0
    rows: 0
    iterator:
      lookup: 0
      get:
        rows: 0
        bytes: 0
  memory:
    bytes: 211783
    index_size: 49152
    rows: 206
    iterator:
      lookup: 0
      get:
        rows: 0
        bytes: 0
  get:
    rows: 0
    bytes: 0
...
gstat()
---
- tx:
    conflict: 0
    commit: 0
    rollback: 0
    statements: 0
    transactions: 0
    gap_locks: 0
    read_views: 0
  memory:
    tuple_cache: 14313
    tx: 0
    bloom_filter: 140
    page_index: 1250
    tuple: 13689
  disk:
    data_compacted: 104299
    data: 104299
    index: 1390
  scheduler:
    tasks_inprogress: 0
    dump_output: 0
    compaction_queue: 0
    compaction_output: 0
    dump_count: 0
    tasks_failed: 0
    tasks_completed: 0
    dump_input: 0
    compaction_input: 0
...
s:drop()
---
...
-- sched stats
s = box.schema.space.create('test', {engine = 'vinyl'})
---
...
i1 = s:create_index('i1', {parts = {1, 'unsigned'}})
---
...
i2 = s:create_index('i2', {parts = {2, 'unsigned'}, unique = false})
---
...
for i = 1, 100 do s:replace{i, i, string.rep('x', 1000)} end
---
...
st = gstat()
---
...
box.snapshot()
---
- ok
...
stat_diff(gstat(), st, 'scheduler')
---
- dump_input: 103400
  dump_output: 103592
  tasks_completed: 2
  dump_count: 1
...
for i = 1, 100, 10 do s:replace{i, i, string.rep('y', 1000)} end
---
...
st = gstat()
---
...
box.snapshot()
---
- ok
...
stat_diff(gstat(), st, 'scheduler')
---
- dump_input: 10340
  dump_output: 10411
  tasks_completed: 2
  dump_count: 1
...
st = gstat()
---
...
i1:compact()
---
...
while i1:stat().disk.compaction.count == 0 do fiber.sleep(0.01) end
---
...
stat_diff(gstat(), st, 'scheduler')
---
- compaction_input: 112228
  tasks_completed: 1
  compaction_output: 101984
...
st = gstat()
---
...
i2:compact()
---
...
while i2:stat().disk.compaction.count == 0 do fiber.sleep(0.01) end
---
...
stat_diff(gstat(), st, 'scheduler')
---
- compaction_input: 1775
  tasks_completed: 1
  compaction_output: 1608
...
s:drop()
---
...
--
-- space.bsize, index.len, index.bsize
--
s = box.schema.space.create('test', {engine = 'vinyl'})
---
...
s:bsize()
---
- 0
...
i1 = s:create_index('i1', {parts = {1, 'unsigned'}, run_count_per_level = 10})
---
...
i2 = s:create_index('i2', {parts = {2, 'unsigned'}, run_count_per_level = 10})
---
...
s:bsize()
---
- 0
...
i1:len(), i2:len()
---
- 0
- 0
...
i1:bsize(), i2:bsize()
---
- 0
- 0
...
for i = 1, 100, 2 do s:replace{i, i, pad()} end
---
...
gst = gstat()
---
...
st1 = i1:stat()
---
...
st2 = i2:stat()
---
...
s:bsize()
---
- 52900
...
i1:len(), i2:len()
---
- 50
- 50
...
i1:bsize(), i2:bsize()
---
- 49152
- 49152
...
s:bsize() == st1.memory.bytes
---
- true
...
i1:len() == st1.memory.rows
---
- true
...
i2:len() == st2.memory.rows
---
- true
...
i1:bsize() == st1.memory.index_size
---
- true
...
i2:bsize() == st2.memory.index_size
---
- true
...
gst.memory.page_index == 0
---
- true
...
gst.memory.bloom_filter == 0
---
- true
...
gst.disk.data == 0
---
- true
...
gst.disk.index == 0
---
- true
...
box.snapshot()
---
- ok
...
gst = gstat()
---
...
st1 = i1:stat()
---
...
st2 = i2:stat()
---
...
s:bsize()
---
- 52199
...
i1:len(), i2:len()
---
- 50
- 50
...
i1:bsize(), i2:bsize()
---
- 420
- 928
...
s:bsize() == st1.disk.bytes
---
- true
...
i1:len() == st1.disk.rows
---
- true
...
i2:len() == st2.disk.rows
---
- true
...
i1:bsize() == st1.disk.index_size + st1.disk.bloom_size
---
- true
...
i2:bsize() == st2.disk.index_size + st2.disk.bloom_size + st2.disk.bytes
---
- true
...
gst.memory.page_index == st1.disk.index_size + st2.disk.index_size
---
- true
...
gst.memory.bloom_filter == st1.disk.bloom_size + st2.disk.bloom_size
---
- true
...
gst.disk.data == s:bsize()
---
- true
...
gst.disk.index == i1:bsize() + i2:bsize()
---
- true
...
for i = 1, 100, 2 do s:delete(i) end
---
...
for i = 2, 100, 2 do s:replace{i, i, pad()} end
---
...
st1 = i1:stat()
---
...
st2 = i2:stat()
---
...
s:bsize()
---
- 106399
...
i1:len(), i2:len()
---
- 150
- 100
...
i1:bsize(), i2:bsize()
---
- 49572
- 50080
...
s:bsize() == st1.memory.bytes + st1.disk.bytes
---
- true
...
i1:len() == st1.memory.rows + st1.disk.rows
---
- true
...
i2:len() == st2.memory.rows + st2.disk.rows
---
- true
...
i1:bsize() == st1.memory.index_size + st1.disk.index_size + st1.disk.bloom_size
---
- true
...
i2:bsize() == st2.memory.index_size + st2.disk.index_size + st2.disk.bloom_size + st2.disk.bytes
---
- true
...
-- Compact the primary index first to generate deferred DELETEs.
-- Then dump them and compact the secondary index.
box.snapshot()
---
- ok
...
i1:compact()
---
...
while i1:stat().run_count > 1 do fiber.sleep(0.01) end
---
...
box.snapshot()
---
- ok
...
i2:compact()
---
...
while i2:stat().run_count > 1 do fiber.sleep(0.01) end
---
...
gst = gstat()
---
...
st1 = i1:stat()
---
...
st2 = i2:stat()
---
...
s:bsize()
---
- 52199
...
i1:len(), i2:len()
---
- 50
- 50
...
i1:bsize(), i2:bsize()
---
- 420
- 928
...
s:bsize() == st1.disk.bytes
---
- true
...
i1:len() == st1.disk.rows
---
- true
...
i2:len() == st2.disk.rows
---
- true
...
i1:bsize() == st1.disk.index_size + st1.disk.bloom_size
---
- true
...
i2:bsize() == st2.disk.index_size + st2.disk.bloom_size + st2.disk.bytes
---
- true
...
gst.memory.page_index == st1.disk.index_size + st2.disk.index_size
---
- true
...
gst.memory.bloom_filter == st1.disk.bloom_size + st2.disk.bloom_size
---
- true
...
gst.disk.data == s:bsize()
---
- true
...
gst.disk.index == i1:bsize() + i2:bsize()
---
- true
...
s:drop()
---
...
gst = gstat()
---
...
gst.memory.page_index == 0
---
- true
...
gst.memory.bloom_filter == 0
---
- true
...
gst.disk.data == 0
---
- true
...
gst.disk.index == 0
---
- true
...
--
-- Statement statistics.
--
s = box.schema.space.create('test', {engine = 'vinyl'})
---
...
i = s:create_index('primary', {run_count_per_level = 10})
---
...
i:stat().disk.statement
---
- inserts: 0
  replaces: 0
  upserts: 0
  deletes: 0
...
-- First run ought to be big to avoid last-level compaction (see gh-3657).
digest = require('digest')
---
...
_ = s:insert{1, 1, digest.urandom(100)}
---
...
_ = s:replace{2, 2, digest.urandom(100)}
---
...
box.snapshot()
---
- ok
...
i:stat().disk.statement
---
- inserts: 1
  replaces: 1
  upserts: 0
  deletes: 0
...
s:upsert({1, 1}, {{'+', 2, 1}})
---
...
s:delete{2}
---
...
box.snapshot()
---
- ok
...
i:stat().disk.statement
---
- inserts: 1
  replaces: 1
  upserts: 1
  deletes: 1
...
test_run:cmd('restart server test')
fiber = require('fiber')
---
...
digest = require('digest')
---
...
s = box.space.test
---
...
i = s.index.primary
---
...
i:stat().disk.statement
---
- inserts: 1
  replaces: 1
  upserts: 1
  deletes: 1
...
i:compact()
---
...
while i:stat().run_count > 1 do fiber.sleep(0.01) end
---
...
i:stat().disk.statement
---
- inserts: 1
  replaces: 0
  upserts: 0
  deletes: 0
...
s:drop()
---
...
--
-- Last level size.
--
s = box.schema.space.create('test', {engine = 'vinyl'})
---
...
i1 = s:create_index('i1', {parts = {1, 'unsigned'}})
---
...
i2 = s:create_index('i2', {parts = {2, 'unsigned'}})
---
...
i1:stat().disk.last_level
---
- bytes_compressed: <bytes_compressed>
  pages: 0
  rows: 0
  bytes: 0
...
i2:stat().disk.last_level
---
- bytes_compressed: <bytes_compressed>
  pages: 0
  rows: 0
  bytes: 0
...
box.stat.vinyl().disk.data_compacted
---
- 0
...
for i = 1, 100 do s:replace{i, i, digest.urandom(100)} end
---
...
box.snapshot()
---
- ok
...
i1:stat().disk.last_level
---
- bytes_compressed: <bytes_compressed>
  pages: 2
  rows: 100
  bytes: 11815
...
i2:stat().disk.last_level
---
- bytes_compressed: <bytes_compressed>
  pages: 1
  rows: 100
  bytes: 1608
...
box.stat.vinyl().disk.data_compacted
---
- 11815
...
for i = 1, 100, 10 do s:replace{i, i * 1000, digest.urandom(100)} end
---
...
box.snapshot()
---
- ok
...
i1:stat().disk.last_level
---
- bytes_compressed: <bytes_compressed>
  pages: 2
  rows: 100
  bytes: 11815
...
i2:stat().disk.last_level
---
- bytes_compressed: <bytes_compressed>
  pages: 1
  rows: 100
  bytes: 1608
...
box.stat.vinyl().disk.data_compacted
---
- 11815
...
i1:compact()
---
...
while i1:stat().disk.compaction.count == 0 do fiber.sleep(0.01) end
---
...
i1:stat().disk.last_level
---
- bytes_compressed: <bytes_compressed>
  pages: 2
  rows: 100
  bytes: 11841
...
box.stat.vinyl().disk.data_compacted
---
- 11841
...
i2:compact()
---
...
while i2:stat().disk.compaction.count == 0 do fiber.sleep(0.01) end
---
...
i2:stat().disk.last_level
---
- bytes_compressed: <bytes_compressed>
  pages: 1
  rows: 110
  bytes: 1794
...
box.stat.vinyl().disk.data_compacted
---
- 11841
...
s:drop()
---
...
box.stat.vinyl().disk.data_compacted
---
- 0
...
--
-- Number of dumps needed to trigger major compaction in
-- an LSM tree range.
--
s = box.schema.space.create('test', {engine = 'vinyl'})
---
...
i = s:create_index('primary', {page_size = 128, range_size = 8192, run_count_per_level = 1, run_size_ratio = 2})
---
...
test_run:cmd("setopt delimiter ';'")
---
- true
...
function dump(a, b)
    for i = a, b do
        s:replace{i, digest.urandom(100)}
    end
    box.snapshot()
end;
---
...
function wait_compaction(count)
    test_run:wait_cond(function()
        return i:stat().disk.compaction.count == count
    end, 10)
end;
---
...
test_run:cmd("setopt delimiter ''");
---
- true
...
dump(1, 100)
---
...
i:stat().dumps_per_compaction -- 1
---
- 1
...
dump(1, 100) -- compaction
---
...
dump(1, 100) -- split + compaction
---
...
wait_compaction(3)
---
...
i:stat().range_count -- 2
---
- 2
...
i:stat().dumps_per_compaction -- 1
---
- 1
...
dump(1, 10)
---
...
dump(1, 40) -- compaction in range 1
---
...
wait_compaction(4)
---
...
i:stat().dumps_per_compaction -- 1
---
- 1
...
dump(90, 100)
---
...
dump(60, 100) -- compaction in range 2
---
...
wait_compaction(5)
---
...
i:stat().dumps_per_compaction -- 2
---
- 2
...
-- Forcing compaction manually doesn't affect dumps_per_compaction.
dump(40, 60)
---
...
i:compact()
---
...
wait_compaction(7)
---
...
i:stat().dumps_per_compaction -- 2
---
- 2
...
test_run:cmd('restart server test')
fiber = require('fiber')
---
...
digest = require('digest')
---
...
s = box.space.test
---
...
i = s.index.primary
---
...
i:stat().dumps_per_compaction -- 2
---
- 2
...
for i = 1, 100 do s:replace{i, digest.urandom(100)} end
---
...
box.snapshot()
---
- ok
...
test_run:wait_cond(function() return i:stat().disk.compaction.count == 2 end, 10)
---
- true
...
i:stat().dumps_per_compaction -- 1
---
- 1
...
s:drop()
---
...
--
-- Check that index.stat.txw.rows is unaccounted on rollback
-- to a savepoint.
--
s = box.schema.space.create('test', {engine = 'vinyl'})
---
...
i = s:create_index('pk')
---
...
box.begin()
---
...
s:insert{1}
---
- [1]
...
i:stat().txw.rows -- 1
---
- 1
...
sv = box.savepoint()
---
...
s:insert{2}
---
- [2]
...
i:stat().txw.rows -- 2
---
- 2
...
box.rollback_to_savepoint(sv)
---
...
i:stat().txw.rows -- 1
---
- 1
...
box.commit()
---
...
i:stat().txw.rows -- 0
---
- 0
...
s:drop()
---
...
test_run:cmd('switch default')
---
- true
...
test_run:cmd('stop server test')
---
- true
...
test_run:cmd('cleanup server test')
---
- true
...
test_run:cmd("clear filter")
---
- true
...
